import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from datetime import datetime, timedelta
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import japanize_matplotlib

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8')

class FactoryDataExplorer:
    """å·¥å ´ãƒ‡ãƒ¼ã‚¿ã®åŒ…æ‹¬çš„ãªæ¢ç´¢åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, data_path):
        """
        ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åˆæœŸåŒ–
        
        Args:
            data_path (str): CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.data = pd.read_csv(data_path)
        self.original_data = self.data.copy()
        self.findings = []  # ç™ºè¦‹äº‹é …ã‚’è¨˜éŒ²
        
        # æ—¥æ™‚åˆ—ã®è‡ªå‹•æ¤œå‡ºãƒ»å¤‰æ›
        self._detect_datetime_columns()
        
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {self.data.shape[0]}è¡Œ x {self.data.shape[1]}åˆ—")
        
    def _detect_datetime_columns(self):
        """æ—¥æ™‚åˆ—ã®è‡ªå‹•æ¤œå‡ºã¨å¤‰æ›"""
        datetime_candidates = []
        
        for col in self.data.columns:
            if self.data[col].dtype == 'object':
                try:
                    # æ—¥æ™‚å¤‰æ›ã‚’è©¦è¡Œ
                    pd.to_datetime(self.data[col].head(100))
                    datetime_candidates.append(col)
                except:
                    continue
        
        if datetime_candidates:
            print(f"æ—¥æ™‚åˆ—å€™è£œ: {datetime_candidates}")
            for col in datetime_candidates:
                try:
                    self.data[col] = pd.to_datetime(self.data[col])
                    print(f"  {col} ã‚’æ—¥æ™‚å‹ã«å¤‰æ›ã—ã¾ã—ãŸ")
                except:
                    continue
    
    def basic_info(self):
        """åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿æƒ…å ±ã®è¡¨ç¤º"""
        print("=" * 60)
        print("ã€åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã€‘")
        print("=" * 60)
        
        # ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶
        print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {self.data.shape}")
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {self.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        print()
        
        # åˆ—æƒ…å ±
        print("ã€åˆ—æƒ…å ±ã€‘")
        info_df = pd.DataFrame({
            'åˆ—å': self.data.columns,
            'ãƒ‡ãƒ¼ã‚¿å‹': self.data.dtypes,
            'æ¬ æå€¤æ•°': self.data.isnull().sum(),
            'æ¬ æç‡(%)': (self.data.isnull().sum() / len(self.data) * 100).round(2),
            'ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°': [self.data[col].nunique() for col in self.data.columns],
            'ã‚µãƒ³ãƒ—ãƒ«å€¤': [str(self.data[col].iloc[0])[:20] for col in self.data.columns]
        })
        print(info_df.to_string(index=False))
        
        # ãƒ‡ãƒ¼ã‚¿å‹åˆ¥ã‚µãƒãƒªãƒ¼
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        categorical_cols = self.data.select_dtypes(include=['object']).columns
        datetime_cols = self.data.select_dtypes(include=['datetime64']).columns
        
        print(f"\næ•°å€¤åˆ—: {len(numeric_cols)}å€‹")
        print(f"ã‚«ãƒ†ã‚´ãƒªåˆ—: {len(categorical_cols)}å€‹") 
        print(f"æ—¥æ™‚åˆ—: {len(datetime_cols)}å€‹")
        
        return info_df
    
    def statistical_summary(self):
        """çµ±è¨ˆçš„ã‚µãƒãƒªãƒ¼ã®è©³ç´°åˆ†æ"""
        print("\n" + "=" * 60)
        print("ã€çµ±è¨ˆçš„ã‚µãƒãƒªãƒ¼ã€‘")
        print("=" * 60)
        
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) == 0:
            print("æ•°å€¤åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # åŸºæœ¬çµ±è¨ˆé‡
        stats_df = self.data[numeric_cols].describe()
        print("åŸºæœ¬çµ±è¨ˆé‡:")
        print(stats_df.round(3))
        
        # è¿½åŠ çµ±è¨ˆé‡
        additional_stats = pd.DataFrame(index=numeric_cols)
        additional_stats['æ­ªåº¦'] = self.data[numeric_cols].skew()
        additional_stats['å°–åº¦'] = self.data[numeric_cols].kurtosis()
        additional_stats['å¤‰å‹•ä¿‚æ•°'] = self.data[numeric_cols].std() / self.data[numeric_cols].mean()
        additional_stats['ã‚¼ãƒ­å€¤æ•°'] = (self.data[numeric_cols] == 0).sum()
        additional_stats['è² å€¤æ•°'] = (self.data[numeric_cols] < 0).sum()
        
        print("\nè¿½åŠ çµ±è¨ˆé‡:")
        print(additional_stats.round(3))
        
        # ç‰¹å¾´çš„ãªç™ºè¦‹ã®è¨˜éŒ²
        for col in numeric_cols:
            skew_val = self.data[col].skew()
            if abs(skew_val) > 2:
                self.findings.append(f"{col}: å¼·ã„æ­ªã¿ (æ­ªåº¦={skew_val:.3f})")
            
            cv = self.data[col].std() / self.data[col].mean()
            if cv > 1:
                self.findings.append(f"{col}: é«˜ã„å¤‰å‹•æ€§ (å¤‰å‹•ä¿‚æ•°={cv:.3f})")
        
        return stats_df, additional_stats
    
    def missing_value_analysis(self):
        """æ¬ æå€¤ã®è©³ç´°åˆ†æ"""
        print("\n" + "=" * 60)
        print("ã€æ¬ æå€¤åˆ†æã€‘")
        print("=" * 60)
        
        missing_summary = pd.DataFrame({
            'åˆ—å': self.data.columns,
            'æ¬ æå€¤æ•°': self.data.isnull().sum(),
            'æ¬ æç‡(%)': (self.data.isnull().sum() / len(self.data) * 100).round(2)
        })
        missing_summary = missing_summary[missing_summary['æ¬ æå€¤æ•°'] > 0].sort_values('æ¬ æç‡(%)', ascending=False)
        
        if len(missing_summary) == 0:
            print("æ¬ æå€¤ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        print("æ¬ æå€¤ã‚µãƒãƒªãƒ¼:")
        print(missing_summary.to_string(index=False))
        
        # æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        if len(missing_summary) > 1:
            print("\næ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ:")
            missing_patterns = self.data[missing_summary['åˆ—å']].isnull()
            pattern_counts = missing_patterns.value_counts()
            print(pattern_counts.head(10))
        
        # æ¬ æå€¤ã®å¯è¦–åŒ–
        if len(missing_summary) > 0:
            plt.figure(figsize=(12, 6))
            
            plt.subplot(1, 2, 1)
            missing_summary.plot(x='åˆ—å', y='æ¬ æç‡(%)', kind='bar', ax=plt.gca())
            plt.title('åˆ—åˆ¥æ¬ æç‡')
            plt.xticks(rotation=45)
            plt.tight_layout()
            
            # æ¬ æå€¤ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
            plt.subplot(1, 2, 2)
            missing_data = self.data[missing_summary['åˆ—å']].isnull()
            if len(missing_data.columns) > 1:
                sns.heatmap(missing_data.corr(), annot=True, cmap='YlOrRd')
                plt.title('æ¬ æå€¤é–“ã®ç›¸é–¢')
            
            plt.tight_layout()
            plt.show()
        
        return missing_summary
    
    def time_series_analysis(self):
        """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ"""
        print("\n" + "=" * 60)
        print("ã€æ™‚ç³»åˆ—åˆ†æã€‘")
        print("=" * 60)
        
        datetime_cols = self.data.select_dtypes(include=['datetime64']).columns
        
        if len(datetime_cols) == 0:
            print("æ—¥æ™‚åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        for time_col in datetime_cols:
            print(f"\n--- {time_col} ã®åˆ†æ ---")
            
            # åŸºæœ¬æƒ…å ±
            print(f"æœŸé–“: {self.data[time_col].min()} ï½ {self.data[time_col].max()}")
            print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(self.data)}ä»¶")
            
            # æ™‚é–“é–“éš”ã®åˆ†æ
            time_diffs = self.data[time_col].diff().dropna()
            print(f"å¹³å‡é–“éš”: {time_diffs.mean()}")
            print(f"æœ€å°é–“éš”: {time_diffs.min()}")
            print(f"æœ€å¤§é–“éš”: {time_diffs.max()}")
            
            # é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ç¢ºèª
            duplicates = self.data[time_col].duplicated().sum()
            if duplicates > 0:
                print(f"é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {duplicates}ä»¶")
                self.findings.append(f"{time_col}: é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒ{duplicates}ä»¶å­˜åœ¨")
            
            # æ™‚é–“é–“éš”ã®ä¸è¦å‰‡æ€§ãƒã‚§ãƒƒã‚¯
            if time_diffs.std() / time_diffs.mean() > 0.1:
                self.findings.append(f"{time_col}: ä¸è¦å‰‡ãªæ™‚é–“é–“éš”")
            
            # æ™‚ç³»åˆ—ã®å¯è¦–åŒ–
            self._plot_time_series_patterns(time_col)
    
    def _plot_time_series_patterns(self, time_col):
        """æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–"""
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) == 0:
            return
        
        # ä»£è¡¨çš„ãªæ•°å€¤åˆ—ã‚’é¸æŠï¼ˆæœ€åˆã®4åˆ—ã¾ã§ï¼‰
        plot_cols = numeric_cols[:4]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.flatten()
        
        for i, col in enumerate(plot_cols):
            if i < len(axes):
                self.data.plot(x=time_col, y=col, ax=axes[i], alpha=0.7)
                axes[i].set_title(f'{col} ã®æ™‚ç³»åˆ—å¤‰åŒ–')
                axes[i].tick_params(axis='x', rotation=45)
        
        # ä½™ã£ãŸè»¸ã‚’éè¡¨ç¤º
        for i in range(len(plot_cols), len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.show()
    
    def outlier_detection(self):
        """å¤–ã‚Œå€¤ã®æ¤œå‡ºã¨åˆ†æ"""
        print("\n" + "=" * 60)
        print("ã€å¤–ã‚Œå€¤æ¤œå‡ºã€‘")
        print("=" * 60)
        
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) == 0:
            print("æ•°å€¤åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        outlier_summary = pd.DataFrame(index=numeric_cols)
        
        for col in numeric_cols:
            data_col = self.data[col].dropna()
            
            # IQRæ³•
            Q1 = data_col.quantile(0.25)
            Q3 = data_col.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            iqr_outliers = ((data_col < lower_bound) | (data_col > upper_bound)).sum()
            
            # Z-scoreæ³•
            z_scores = np.abs(stats.zscore(data_col))
            z_outliers = (z_scores > 3).sum()
            
            outlier_summary.loc[col, 'IQRå¤–ã‚Œå€¤æ•°'] = iqr_outliers
            outlier_summary.loc[col, 'IQRå¤–ã‚Œå€¤ç‡(%)'] = (iqr_outliers / len(data_col) * 100).round(2)
            outlier_summary.loc[col, 'Z-scoreå¤–ã‚Œå€¤æ•°'] = z_outliers
            outlier_summary.loc[col, 'Z-scoreå¤–ã‚Œå€¤ç‡(%)'] = (z_outliers / len(data_col) * 100).round(2)
            
            # ç™ºè¦‹äº‹é …ã®è¨˜éŒ²
            if iqr_outliers / len(data_col) > 0.05:  # 5%ä»¥ä¸Š
                self.findings.append(f"{col}: å¤šæ•°ã®å¤–ã‚Œå€¤ (IQRæ³•ã§{iqr_outliers}ä»¶, {iqr_outliers/len(data_col)*100:.1f}%)")
        
        print("å¤–ã‚Œå€¤ã‚µãƒãƒªãƒ¼:")
        print(outlier_summary)
        
        # å¤–ã‚Œå€¤ã®å¯è¦–åŒ–
        self._plot_outliers(numeric_cols[:6])  # æœ€åˆã®6åˆ—ã¾ã§
        
        return outlier_summary
    
    def _plot_outliers(self, columns):
        """å¤–ã‚Œå€¤ã®å¯è¦–åŒ–"""
        n_cols = len(columns)
        n_rows = (n_cols + 2) // 3
        
        fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))
        if n_rows == 1:
            axes = [axes]
        axes = np.array(axes).flatten()
        
        for i, col in enumerate(columns):
            if i < len(axes):
                self.data.boxplot(column=col, ax=axes[i])
                axes[i].set_title(f'{col} ã®ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ')
        
        # ä½™ã£ãŸè»¸ã‚’éè¡¨ç¤º
        for i in range(len(columns), len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.show()
    
    def correlation_analysis(self):
        """ç›¸é–¢åˆ†æ"""
        print("\n" + "=" * 60)
        print("ã€ç›¸é–¢åˆ†æã€‘")
        print("=" * 60)
        
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) < 2:
            print("ç›¸é–¢åˆ†æã«ååˆ†ãªæ•°å€¤åˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ç›¸é–¢è¡Œåˆ—ã®è¨ˆç®—
        corr_matrix = self.data[numeric_cols].corr()
        
        # é«˜ã„ç›¸é–¢ã‚’æŒã¤ãƒšã‚¢ã®æŠ½å‡º
        high_corr_pairs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.7:  # çµ¶å¯¾å€¤0.7ä»¥ä¸Š
                    high_corr_pairs.append({
                        'å¤‰æ•°1': corr_matrix.columns[i],
                        'å¤‰æ•°2': corr_matrix.columns[j],
                        'ç›¸é–¢ä¿‚æ•°': corr_val
                    })
        
        if high_corr_pairs:
            print("é«˜ã„ç›¸é–¢ã‚’æŒã¤å¤‰æ•°ãƒšã‚¢ (|r| > 0.7):")
            high_corr_df = pd.DataFrame(high_corr_pairs)
            print(high_corr_df.sort_values('ç›¸é–¢ä¿‚æ•°', key=abs, ascending=False))
            
            # ç™ºè¦‹äº‹é …ã®è¨˜éŒ²
            for pair in high_corr_pairs:
                self.findings.append(f"é«˜ç›¸é–¢: {pair['å¤‰æ•°1']} - {pair['å¤‰æ•°2']} (r={pair['ç›¸é–¢ä¿‚æ•°']:.3f})")
        
        # ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        plt.figure(figsize=(12, 10))
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                   square=True, fmt='.2f', cbar_kws={"shrink": .8})
        plt.title('å¤‰æ•°é–“ã®ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—')
        plt.tight_layout()
        plt.show()
        
        return corr_matrix
    
    def distribution_analysis(self):
        """åˆ†å¸ƒã®åˆ†æ"""
        print("\n" + "=" * 60)
        print("ã€åˆ†å¸ƒåˆ†æã€‘")
        print("=" * 60)
        
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) == 0:
            print("æ•°å€¤åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # åˆ†å¸ƒã®å¯è¦–åŒ–
        n_cols = min(len(numeric_cols), 9)  # æœ€å¤§9å€‹ã¾ã§
        n_rows = (n_cols + 2) // 3
        
        fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))
        if n_rows == 1:
            axes = [axes]
        axes = np.array(axes).flatten()
        
        for i, col in enumerate(numeric_cols[:n_cols]):
            data_col = self.data[col].dropna()
            
            # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  + å¯†åº¦æ¨å®š
            axes[i].hist(data_col, bins=50, alpha=0.7, density=True)
            data_col.plot.density(ax=axes[i], color='red', linewidth=2)
            axes[i].set_title(f'{col} ã®åˆ†å¸ƒ')
            axes[i].set_ylabel('å¯†åº¦')
            
            # æ­£è¦æ€§ã®æ¤œå®š
            if len(data_col) > 3:
                _, p_value = stats.shapiro(data_col.sample(min(5000, len(data_col))))
                if p_value < 0.05:
                    self.findings.append(f"{col}: éæ­£è¦åˆ†å¸ƒ (Shapiro-Wilk p={p_value:.3e})")
        
        # ä½™ã£ãŸè»¸ã‚’éè¡¨ç¤º
        for i in range(n_cols, len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.show()
    
    def categorical_analysis(self):
        """ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®åˆ†æ"""
        print("\n" + "=" * 60)
        print("ã€ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°åˆ†æã€‘")
        print("=" * 60)
        
        categorical_cols = self.data.select_dtypes(include=['object']).columns
        datetime_cols = self.data.select_dtypes(include=['datetime64']).columns
        
        # æ—¥æ™‚åˆ—ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ—ã‹ã‚‰é™¤å¤–
        categorical_cols = [col for col in categorical_cols if col not in datetime_cols]
        
        if len(categorical_cols) == 0:
            print("ã‚«ãƒ†ã‚´ãƒªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        for col in categorical_cols:
            print(f"\n--- {col} ã®åˆ†æ ---")
            
            value_counts = self.data[col].value_counts()
            print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•°: {len(value_counts)}")
            print(f"æœ€é »å€¤: {value_counts.index[0]} ({value_counts.iloc[0]}ä»¶)")
            
            # åˆ†å¸ƒã®åã‚Š
            if len(value_counts) > 1:
                concentration = value_counts.iloc[0] / len(self.data)
                if concentration > 0.9:
                    self.findings.append(f"{col}: é«˜åº¦ã«åã£ãŸåˆ†å¸ƒ (æœ€é »å€¤ãŒ{concentration*100:.1f}%)")
            
            # ä¸Šä½10ä»¶ã¾ã§è¡¨ç¤º
            print("å€¤ã®åˆ†å¸ƒ:")
            print(value_counts.head(10))
            
            # å¯è¦–åŒ–ï¼ˆã‚«ãƒ†ã‚´ãƒªæ•°ãŒé©åº¦ãªå ´åˆï¼‰
            if 2 <= len(value_counts) <= 20:
                plt.figure(figsize=(10, 6))
                value_counts.head(10).plot(kind='bar')
                plt.title(f'{col} ã®åˆ†å¸ƒ')
                plt.xticks(rotation=45)
                plt.tight_layout()
                plt.show()
    
    def data_quality_check(self):
        """ãƒ‡ãƒ¼ã‚¿å“è³ªã®ç·åˆãƒã‚§ãƒƒã‚¯"""
        print("\n" + "=" * 60)
        print("ã€ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã€‘")
        print("=" * 60)
        
        quality_issues = []
        
        # 1. é‡è¤‡è¡Œã®ç¢ºèª
        duplicates = self.data.duplicated().sum()
        if duplicates > 0:
            quality_issues.append(f"é‡è¤‡è¡Œ: {duplicates}ä»¶")
        
        # 2. å®Œå…¨ã«åŒã˜å€¤ã‚’æŒã¤åˆ—ã®ç¢ºèª
        constant_cols = []
        for col in self.data.columns:
            if self.data[col].nunique() <= 1:
                constant_cols.append(col)
        
        if constant_cols:
            quality_issues.append(f"å®šæ•°åˆ—: {constant_cols}")
        
        # 3. ç•°å¸¸ã«é«˜ã„/ä½ã„å€¤ã®ç¢ºèª
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            data_col = self.data[col].dropna()
            if len(data_col) > 0:
                # æ¥µç«¯ã«å¤§ããªå€¤
                if data_col.max() > data_col.median() * 1000:
                    quality_issues.append(f"{col}: æ¥µç«¯ã«å¤§ããªå€¤ (æœ€å¤§å€¤={data_col.max():.2e})")
                
                # è² ã®å€¤ï¼ˆç‰©ç†çš„ã«ãŠã‹ã—ã„å ´åˆï¼‰
                if data_col.min() < 0 and 'temp' in col.lower():  # æ¸©åº¦ã®ä¾‹
                    quality_issues.append(f"{col}: è² ã®æ¸©åº¦å€¤")
        
        # 4. æ™‚ç³»åˆ—ã®é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯
        datetime_cols = self.data.select_dtypes(include=['datetime64']).columns
        for time_col in datetime_cols:
            if len(self.data) > 1:
                time_diffs = self.data[time_col].diff().dropna()
                if time_diffs.min() <= pd.Timedelta(0):
                    quality_issues.append(f"{time_col}: æ™‚ç³»åˆ—ã®é€†è»¢ã¾ãŸã¯åœæ­¢")
        
        # çµæœã®è¡¨ç¤º
        if quality_issues:
            print("ç™ºè¦‹ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å“è³ªã®å•é¡Œ:")
            for issue in quality_issues:
                print(f"  âš  {issue}")
                self.findings.append(f"å“è³ªå•é¡Œ: {issue}")
        else:
            print("é‡å¤§ãªãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        return quality_issues
    
    def pattern_detection(self):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        print("\n" + "=" * 60)
        print("ã€ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã€‘")
        print("=" * 60)
        
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) < 2:
            print("ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # PCAåˆ†æ
        data_scaled = StandardScaler().fit_transform(self.data[numeric_cols].fillna(0))
        pca = PCA()
        pca_result = pca.fit_transform(data_scaled)
        
        # ç´¯ç©å¯„ä¸ç‡
        cumsum_ratio = np.cumsum(pca.explained_variance_ratio_)
        n_components_80 = np.argmax(cumsum_ratio >= 0.8) + 1
        n_components_95 = np.argmax(cumsum_ratio >= 0.95) + 1
        
        print(f"ä¸»æˆåˆ†åˆ†æçµæœ:")
        print(f"  80%ã®åˆ†æ•£ã‚’èª¬æ˜ã™ã‚‹ã®ã«å¿…è¦ãªä¸»æˆåˆ†æ•°: {n_components_80}")
        print(f"  95%ã®åˆ†æ•£ã‚’èª¬æ˜ã™ã‚‹ã®ã«å¿…è¦ãªä¸»æˆåˆ†æ•°: {n_components_95}")
        
        # å¯„ä¸ç‡ã®å¯è¦–åŒ–
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        plt.bar(range(1, min(11, len(pca.explained_variance_ratio_)+1)), 
                pca.explained_variance_ratio_[:10])
        plt.title('ä¸»æˆåˆ†ã®å¯„ä¸ç‡')
        plt.xlabel('ä¸»æˆåˆ†')
        plt.ylabel('å¯„ä¸ç‡')
        
        plt.subplot(1, 2, 2)
        plt.plot(range(1, min(11, len(cumsum_ratio)+1)), cumsum_ratio[:10], 'o-')
        plt.axhline(y=0.8, color='r', linestyle='--', label='80%')
        plt.axhline(y=0.95, color='g', linestyle='--', label='95%')
        plt.title('ç´¯ç©å¯„ä¸ç‡')
        plt.xlabel('ä¸»æˆåˆ†æ•°')
        plt.ylabel('ç´¯ç©å¯„ä¸ç‡')
        plt.legend()
        
        plt.tight_layout()
        plt.show()
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ
        if len(self.data) > 10:
            optimal_k = min(8, len(self.data) // 10)
            if optimal_k >= 2:
                kmeans = KMeans(n_clusters=optimal_k, random_state=42)
                clusters = kmeans.fit_predict(data_scaled)
                
                print(f"\nã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ (k={optimal_k}):")
                cluster_counts = pd.Series(clusters).value_counts().sort_index()
                print(cluster_counts)
                
                if optimal_k <= 10:  # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒé©åº¦ãªå ´åˆã®ã¿è¨˜éŒ²
                    self.findings.append(f"ãƒ‡ãƒ¼ã‚¿ã¯{optimal_k}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«åˆ†é¡å¯èƒ½")
        
        return pca, cumsum_ratio
    
    def anomaly_scoring(self):
        """ç•°å¸¸ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        print("\n" + "=" * 60)
        print("ã€ç•°å¸¸åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã€‘")
        print("=" * 60)
        
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) == 0:
            print("æ•°å€¤åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒãƒ«ãƒ†ã‚£ãƒãƒªã‚¨ãƒ¼ãƒˆç•°å¸¸æ¤œå‡º
        from sklearn.ensemble import IsolationForest
        
        # æ¬ æå€¤ã‚’å¹³å‡ã§åŸ‹ã‚ã‚‹
        data_filled = self.data[numeric_cols].fillna(self.data[numeric_cols].mean())
        
        # Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸æ¤œå‡º
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_scores = iso_forest.fit_predict(data_filled)
        anomaly_score_values = iso_forest.score_samples(data_filled)
        
        # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒãƒªãƒ¼
        n_anomalies = (anomaly_scores == -1).sum()
        print(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ãƒ‡ãƒ¼ã‚¿: {n_anomalies}ä»¶ ({n_anomalies/len(self.data)*100:.2f}%)")
        
        # ç•°å¸¸ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
        plt.figure(figsize=(10, 6))
        plt.hist(anomaly_score_values, bins=50, alpha=0.7)
        plt.axvline(x=np.percentile(anomaly_score_values, 10), color='r', 
                   linestyle='--', label='ä¸‹ä½10%é–¾å€¤')
        plt.title('ç•°å¸¸åº¦ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ')
        plt.xlabel('ç•°å¸¸åº¦ã‚¹ã‚³ã‚¢ï¼ˆä½ã„ã»ã©ç•°å¸¸ï¼‰')
        plt.ylabel('é »åº¦')
        plt.legend()
        plt.show()
        
        # æœ€ã‚‚ç•°å¸¸ãªãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’è¡¨ç¤º
        anomaly_indices = np.argsort(anomaly_score_values)[:5]
        print("\næœ€ã‚‚ç•°å¸¸ãªãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆï¼ˆä¸Šä½5ä»¶ï¼‰:")
        for i, idx in enumerate(anomaly_indices):
            print(f"{i+1}. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {idx}: ã‚¹ã‚³ã‚¢ = {anomaly_score_values[idx]:.3f}")
        
        if n_anomalies > len(self.data) * 0.05:  # 5%ä»¥ä¸ŠãŒç•°å¸¸
            self.findings.append(f"å¤šæ•°ã®ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ãŒæ¤œå‡º ({n_anomalies}ä»¶, {n_anomalies/len(self.data)*100:.1f}%)")
        
        return anomaly_scores, anomaly_score_values
    
    def generate_insights_report(self):
        """ç™ºè¦‹äº‹é …ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\n" + "=" * 80)
        print("ã€ç·åˆãƒ¬ãƒãƒ¼ãƒˆ - ç™ºè¦‹äº‹é …ã¨ãƒ’ã‚¢ãƒªãƒ³ã‚°æ¨å¥¨é …ç›®ã€‘")
        print("=" * 80)
        
        if not self.findings:
            print("ç‰¹ç­†ã™ã¹ãç™ºè¦‹äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        print("â–  ãƒ‡ãƒ¼ã‚¿åˆ†æã§ç™ºè¦‹ã•ã‚ŒãŸç‰¹å¾´çš„ãªç‚¹:")
        for i, finding in enumerate(self.findings, 1):
            print(f"{i:2d}. {finding}")
        
        print("\nâ–  ãƒ’ã‚¢ãƒªãƒ³ã‚°æ¨å¥¨é …ç›®:")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æ¨å¥¨è³ªå•ã‚’ç”Ÿæˆ
        questions = []
        
        # æ¬ æå€¤é–¢é€£
        missing_findings = [f for f in self.findings if 'æ¬ æ' in f]
        if missing_findings:
            questions.append("æ¬ æå€¤ã®ç™ºç”ŸåŸå› ã¨å‡¦ç†æ–¹é‡ã«ã¤ã„ã¦")
            questions.append("æ¬ ææœŸé–“ä¸­ã®è¨­å‚™ã®çŠ¶æ³ã«ã¤ã„ã¦")
        
        # å¤–ã‚Œå€¤é–¢é€£
        outlier_findings = [f for f in self.findings if 'å¤–ã‚Œå€¤' in f]
        if outlier_findings:
            questions.append("å¤–ã‚Œå€¤ãŒç™ºç”Ÿã™ã‚‹å…¸å‹çš„ãªåŸå› ã«ã¤ã„ã¦")
            questions.append("è¨­å‚™ç•°å¸¸ã‚„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ä½œæ¥­ã¨ã®é–¢é€£ã«ã¤ã„ã¦")
        
        # ç›¸é–¢é–¢é€£
        corr_findings = [f for f in self.findings if 'ç›¸é–¢' in f]
        if corr_findings:
            questions.append("é«˜ã„ç›¸é–¢ã‚’ç¤ºã™å¤‰æ•°é–“ã®å› æœé–¢ä¿‚ã«ã¤ã„ã¦")
            questions.append("å·¥ç¨‹åˆ¶å¾¡ã®ãƒ­ã‚¸ãƒƒã‚¯ã«ã¤ã„ã¦")
        
        # æ™‚ç³»åˆ—é–¢é€£
        time_findings = [f for f in self.findings if 'æ™‚é–“' in f or 'æ™‚ç³»åˆ—' in f]
        if time_findings:
            questions.append("ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é »åº¦ã®å¤‰æ›´å±¥æ­´ã«ã¤ã„ã¦")
            questions.append("ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ã®ä»•æ§˜ã«ã¤ã„ã¦")
        
        # åˆ†å¸ƒé–¢é€£
        dist_findings = [f for f in self.findings if 'åˆ†å¸ƒ' in f or 'æ­ªã¿' in f]
        if dist_findings:
            questions.append("å¤‰æ•°ã®æ­£å¸¸å€¤ç¯„å›²ã«ã¤ã„ã¦")
            questions.append("é‹è»¢æ¡ä»¶ã‚„è¨­å‚™ã®åˆ¶ç´„ã«ã¤ã„ã¦")
        
        # å“è³ªé–¢é€£
        quality_findings = [f for f in self.findings if 'å“è³ª' in f]
        if quality_findings:
            questions.append("ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»è¨˜éŒ²ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦")
            questions.append("è¨ˆæ¸¬å™¨ã®æ ¡æ­£å±¥æ­´ã«ã¤ã„ã¦")
        
        # ä¸€èˆ¬çš„ãªæ¨å¥¨è³ªå•
        questions.extend([
            "ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚„è£œæ­£ã®æ–¹æ³•ã«ã¤ã„ã¦",
            "ç›®çš„å¤‰æ•°ï¼ˆæ°´åˆ†å€¤ï¼‰ã«å½±éŸ¿ã™ã‚‹ä¸»è¦å› å­ã«ã¤ã„ã¦",
            "30åˆ†ã®é…ã‚Œæ™‚é–“ã®å¦¥å½“æ€§ã«ã¤ã„ã¦",
            "äºˆæ¸¬ç²¾åº¦ã®è¦æ±‚ãƒ¬ãƒ™ãƒ«ã«ã¤ã„ã¦",
            "é‹ç”¨æ™‚ã®æ›´æ–°é »åº¦ã‚„è¨±å®¹ã§ãã‚‹è¨ˆç®—æ™‚é–“ã«ã¤ã„ã¦"
        ])
        
        for i, question in enumerate(questions, 1):
            print(f"{i:2d}. {question}")
        
        print(f"\nç™ºè¦‹äº‹é …ç·æ•°: {len(self.findings)}ä»¶")
        print(f"æ¨å¥¨ãƒ’ã‚¢ãƒªãƒ³ã‚°é …ç›®: {len(questions)}ä»¶")
    
    def run_comprehensive_analysis(self):
        """åŒ…æ‹¬çš„ãªåˆ†æã®å®Ÿè¡Œ"""
        print("ğŸ” å·¥å ´ãƒ‡ãƒ¼ã‚¿ã®åŒ…æ‹¬çš„æ¢ç´¢åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        
        # é †æ¬¡åˆ†æã‚’å®Ÿè¡Œ
        self.basic_info()
        self.statistical_summary()
        self.missing_value_analysis()
        self.time_series_analysis()
        self.outlier_detection()
        self.correlation_analysis()
        self.distribution_analysis()
        self.categorical_analysis()
        self.data_quality_check()
        self.pattern_detection()
        self.anomaly_scoring()
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        self.generate_insights_report()
        
        print("\nâœ… åŒ…æ‹¬çš„åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
        return self.findings

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
    data_path = "factory_data.csv"  # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´ã—ã¦ãã ã•ã„
    
    try:
        # åˆ†æå™¨ã®åˆæœŸåŒ–
        explorer = FactoryDataExplorer(data_path)
        
        # åŒ…æ‹¬çš„åˆ†æã®å®Ÿè¡Œ
        findings = explorer.run_comprehensive_analysis()
        
        print(f"\nğŸ“Š åˆ†æå®Œäº†: {len(findings)}å€‹ã®ç™ºè¦‹äº‹é …ãŒè¨˜éŒ²ã•ã‚Œã¾ã—ãŸ")
        
    except FileNotFoundError:
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ« '{data_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("æ­£ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}") 