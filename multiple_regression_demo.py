#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡å›å¸°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’éç¨‹å¯è¦–åŒ–ãƒ‡ãƒ¢

è¤‡æ•°ã®èª¬æ˜å¤‰æ•°ã«ã‚ˆã‚‹é‡å›å¸°ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã¨ä¿‚æ•°ã®é‡è¦åº¦ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚
"""

import numpy as np
import matplotlib.pyplot as plt
import japanize_matplotlib
import pandas as pd
import warnings
import os
from mpl_toolkits.mplot3d import Axes3D
warnings.filterwarnings('ignore')

class MultipleRegressionDemo:
    """é‡å›å¸°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’éç¨‹ã®ãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.history = []
        self.X = None
        self.y = None
        self.coefficients = None
        self.intercept = None
        self.feature_names = None
        
    def generate_sample_data(self, n_samples=100, noise=0.3, seed=42):
        """å·¥å ´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ¡ãƒ¼ã‚¸ã—ãŸã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        np.random.seed(seed)
        
        # èª¬æ˜å¤‰æ•°ï¼ˆå·¥å ´ã®è¦å› ï¼‰
        self.feature_names = ['æ¸©åº¦', 'åœ§åŠ›', 'æµé‡', 'è§¦åª’æ¿ƒåº¦']
        
        # çœŸã®é–¢ä¿‚: y = 2.0*æ¸©åº¦ + 1.5*åœ§åŠ› + 0.8*æµé‡ + 3.0*è§¦åª’æ¿ƒåº¦ + 10.0 + noise
        self.true_coefficients = [2.0, 1.5, 0.8, 3.0]
        self.true_intercept = 10.0
        
        # ç‰¹å¾´é‡ç”Ÿæˆï¼ˆæ¨™æº–åŒ–æ¸ˆã¿ï¼‰
        self.X = np.random.normal(0, 1, (n_samples, 4))
        
        # ç›®çš„å¤‰æ•°è¨ˆç®—
        self.y = (self.X @ self.true_coefficients + self.true_intercept + 
                 np.random.normal(0, noise, n_samples))
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–
        self.coefficients = np.random.uniform(-1, 1, 4)
        self.intercept = np.random.uniform(-1, 1)
        
        print(f"çœŸã®ä¿‚æ•°: {dict(zip(self.feature_names, self.true_coefficients))}")
        print(f"çœŸã®åˆ‡ç‰‡: {self.true_intercept}")
        print(f"åˆæœŸä¿‚æ•°: {dict(zip(self.feature_names, self.coefficients))}")
        print(f"åˆæœŸåˆ‡ç‰‡: {self.intercept:.3f}")
        
    def predict(self, X=None, coefficients=None, intercept=None):
        """äºˆæ¸¬"""
        if X is None:
            X = self.X
        if coefficients is None:
            coefficients = self.coefficients
        if intercept is None:
            intercept = self.intercept
        return X @ coefficients + intercept
    
    def compute_loss(self, coefficients=None, intercept=None):
        """å¹³å‡äºŒä¹—èª¤å·®ã‚’è¨ˆç®—"""
        if coefficients is None:
            coefficients = self.coefficients
        if intercept is None:
            intercept = self.intercept
        
        y_pred = self.predict(self.X, coefficients, intercept)
        loss = np.mean((self.y - y_pred) ** 2)
        return loss
    
    def compute_gradients(self):
        """å‹¾é…ã‚’è¨ˆç®—"""
        y_pred = self.predict()
        error = y_pred - self.y
        n_samples = len(self.y)
        
        # å„ä¿‚æ•°ã®å‹¾é…
        dw = (2/n_samples) * (self.X.T @ error)
        # åˆ‡ç‰‡ã®å‹¾é…
        db = (2/n_samples) * np.sum(error)
        
        return dw, db
    
    def train_step(self, learning_rate=0.01):
        """1ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’"""
        # å‹¾é…è¨ˆç®—
        dw, db = self.compute_gradients()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
        self.coefficients -= learning_rate * dw
        self.intercept -= learning_rate * db
        
        # å±¥æ­´ä¿å­˜
        loss = self.compute_loss()
        self.history.append({
            'coefficients': self.coefficients.copy(),
            'intercept': self.intercept,
            'loss': loss,
            'dw': dw.copy(),
            'db': db
        })
        
        return loss
    
    def train(self, epochs=200, learning_rate=0.01):
        """å­¦ç¿’å®Ÿè¡Œ"""
        self.history = []
        
        for epoch in range(epochs):
            loss = self.train_step(learning_rate)
            
            if epoch % 40 == 0:
                print(f"Epoch {epoch:3d}: Loss={loss:.4f}")
        
        print(f"æœ€çµ‚ä¿‚æ•°: {dict(zip(self.feature_names, self.coefficients))}")
        print(f"æœ€çµ‚åˆ‡ç‰‡: {self.intercept:.3f}")

def plot_multiple_regression_comprehensive():
    """é‡å›å¸°ã®åŒ…æ‹¬çš„ãªå¯è¦–åŒ–"""
    model = MultipleRegressionDemo()
    model.generate_sample_data(n_samples=100, noise=0.5)
    
    # å­¦ç¿’å®Ÿè¡Œ
    model.train(epochs=200, learning_rate=0.1)
    
    # imagesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs('images', exist_ok=True)
    
    # å¤§ããªå›³ã‚’ä½œæˆ
    fig = plt.figure(figsize=(20, 15))
    
    # 1. äºˆæ¸¬ vs å®Ÿæ¸¬å€¤
    ax1 = plt.subplot(2, 3, 1)
    y_pred = model.predict()
    ax1.scatter(model.y, y_pred, alpha=0.6, color='blue')
    
    # å®Œå…¨äºˆæ¸¬ç·š
    min_val = min(model.y.min(), y_pred.min())
    max_val = max(model.y.max(), y_pred.max())
    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
    
    ax1.set_xlabel('å®Ÿæ¸¬å€¤', fontsize=12)
    ax1.set_ylabel('äºˆæ¸¬å€¤', fontsize=12)
    ax1.set_title('1. äºˆæ¸¬ç²¾åº¦', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    # RÂ²ã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º
    ss_res = np.sum((model.y - y_pred) ** 2)
    ss_tot = np.sum((model.y - np.mean(model.y)) ** 2)
    r2 = 1 - (ss_res / ss_tot)
    ax1.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=ax1.transAxes, 
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    # 2. ä¿‚æ•°ã®é‡è¦åº¦ï¼ˆæœ€çµ‚çµæœï¼‰
    ax2 = plt.subplot(2, 3, 2)
    colors = ['red', 'orange', 'green', 'purple']
    bars = ax2.bar(model.feature_names, model.coefficients, color=colors, alpha=0.7)
    
    # çœŸã®ä¿‚æ•°ã‚’ç‚¹ç·šã§è¡¨ç¤º
    for i, (name, true_coef) in enumerate(zip(model.feature_names, model.true_coefficients)):
        ax2.axhline(y=true_coef, xmin=i/4-0.1, xmax=i/4+0.1, 
                   color='black', linestyle='--', linewidth=2)
    
    ax2.set_ylabel('ä¿‚æ•°ã®å€¤', fontsize=12)
    ax2.set_title('2. å„è¦å› ã®å½±éŸ¿åº¦ï¼ˆä¿‚æ•°ï¼‰', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')
    ax2.tick_params(axis='x', rotation=45)
    
    # ä¿‚æ•°ã®å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, coef in zip(bars, model.coefficients):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                f'{coef:.2f}', ha='center', va='bottom', fontweight='bold')
    
    # 3. æå¤±é–¢æ•°ã®å¤‰åŒ–
    ax3 = plt.subplot(2, 3, 3)
    losses = [h['loss'] for h in model.history]
    ax3.plot(losses, 'b-', linewidth=2)
    ax3.set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
    ax3.set_ylabel('æå¤± (MSE)', fontsize=12)
    ax3.set_title('3. å­¦ç¿’ã®é€²è¡Œ', fontsize=14, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    ax3.set_yscale('log')
    
    # 4. ä¿‚æ•°ã®åæŸéç¨‹
    ax4 = plt.subplot(2, 3, 4)
    for i, (name, color) in enumerate(zip(model.feature_names, colors)):
        coef_history = [h['coefficients'][i] for h in model.history]
        ax4.plot(coef_history, color=color, linewidth=2, label=name)
        ax4.axhline(y=model.true_coefficients[i], color=color, 
                   linestyle='--', alpha=0.7)
    
    ax4.set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
    ax4.set_ylabel('ä¿‚æ•°ã®å€¤', fontsize=12)
    ax4.set_title('4. ä¿‚æ•°ã®å­¦ç¿’éç¨‹', fontsize=14, fontweight='bold')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. æ®‹å·®åˆ†æ
    ax5 = plt.subplot(2, 3, 5)
    residuals = model.y - y_pred
    ax5.scatter(y_pred, residuals, alpha=0.6, color='green')
    ax5.axhline(y=0, color='red', linestyle='--', linewidth=2)
    ax5.set_xlabel('äºˆæ¸¬å€¤', fontsize=12)
    ax5.set_ylabel('æ®‹å·®', fontsize=12)
    ax5.set_title('5. æ®‹å·®åˆ†æ', fontsize=14, fontweight='bold')
    ax5.grid(True, alpha=0.3)
    
    # 6. æ•°å¼ã®èª¬æ˜
    ax6 = plt.subplot(2, 3, 6)
    ax6.axis('off')
    
    # å­¦ç¿’ã•ã‚ŒãŸå¼ã‚’è¡¨ç¤º
    equation_parts = []
    for name, coef in zip(model.feature_names, model.coefficients):
        if coef >= 0:
            equation_parts.append(f"+ {coef:.2f}Ã—{name}")
        else:
            equation_parts.append(f"- {abs(coef):.2f}Ã—{name}")
    
    equation = f"äºˆæ¸¬å€¤ = {model.intercept:.2f} " + " ".join(equation_parts)
    
    formula_text = f"""
é‡å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’çµæœ

{equation}

å„ä¿‚æ•°ã®æ„å‘³:
â€¢ å¤§ãã„ä¿‚æ•° â†’ ãã®è¦å› ã®å½±éŸ¿ãŒå¤§ãã„
â€¢ æ­£ã®ä¿‚æ•° â†’ è¦å› ãŒå¢—ãˆã‚‹ã¨äºˆæ¸¬å€¤ã‚‚å¢—åŠ 
â€¢ è² ã®ä¿‚æ•° â†’ è¦å› ãŒå¢—ãˆã‚‹ã¨äºˆæ¸¬å€¤ã¯æ¸›å°‘

RÂ² = {r2:.3f} (1.0ã«è¿‘ã„ã»ã©ç²¾åº¦ãŒé«˜ã„)
    """
    
    ax6.text(0.05, 0.95, formula_text, transform=ax6.transAxes, fontsize=11,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.8))
    
    plt.tight_layout()
    
    # ç”»åƒã‚’ä¿å­˜
    filename = 'images/multiple_regression_comprehensive.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“ åŒ…æ‹¬çš„ãªé‡å›å¸°åˆ†æå›³ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
    
    plt.show()
    
    return model

def plot_simple_multiple_regression():
    """ã‚·ãƒ³ãƒ—ãƒ«ãªé‡å›å¸°çµæœè¡¨ç¤º"""
    model = MultipleRegressionDemo()
    model.generate_sample_data(n_samples=100, noise=0.5)
    
    # å­¦ç¿’å®Ÿè¡Œ
    model.train(epochs=200, learning_rate=0.1)
    
    # imagesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs('images', exist_ok=True)
    
    # å›³ã‚’ä½œæˆ
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    # å·¦å´: äºˆæ¸¬ vs å®Ÿæ¸¬å€¤
    y_pred = model.predict()
    ax1.scatter(model.y, y_pred, alpha=0.7, color='blue', s=60)
    
    # å®Œå…¨äºˆæ¸¬ç·š
    min_val = min(model.y.min(), y_pred.min())
    max_val = max(model.y.max(), y_pred.max())
    ax1.plot([min_val, max_val], [min_val, max_val], 'r-', linewidth=3)
    
    ax1.set_xlabel('å®Ÿæ¸¬å€¤', fontsize=14)
    ax1.set_ylabel('äºˆæ¸¬å€¤', fontsize=14)
    ax1.set_title('äºˆæ¸¬ç²¾åº¦', fontsize=16, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    # RÂ²ã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º
    ss_res = np.sum((model.y - y_pred) ** 2)
    ss_tot = np.sum((model.y - np.mean(model.y)) ** 2)
    r2 = 1 - (ss_res / ss_tot)
    ax1.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=ax1.transAxes, fontsize=12,
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    # å³å´: ä¿‚æ•°ã®é‡è¦åº¦
    colors = ['red', 'orange', 'green', 'purple']
    bars = ax2.bar(model.feature_names, np.abs(model.coefficients), 
                   color=colors, alpha=0.7)
    
    ax2.set_ylabel('ä¿‚æ•°ã®çµ¶å¯¾å€¤', fontsize=14)
    ax2.set_title('å„è¦å› ã®å½±éŸ¿åº¦', fontsize=16, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')
    ax2.tick_params(axis='x', rotation=45)
    
    # ä¿‚æ•°ã®å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, coef in zip(bars, model.coefficients):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                f'{coef:.2f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    
    # ç”»åƒã‚’ä¿å­˜
    filename = 'images/multiple_regression_simple.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“ ã‚·ãƒ³ãƒ—ãƒ«ãªé‡å›å¸°åˆ†æå›³ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
    
    plt.show()
    
    return model

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ“ é‡å›å¸°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ‡ãƒ¢")
    print("=" * 50)
    
    print("\n1. åŒ…æ‹¬çš„ãªé‡å›å¸°åˆ†æ")
    model1 = plot_multiple_regression_comprehensive()
    
    print("\n2. ã‚·ãƒ³ãƒ—ãƒ«ãªé‡å›å¸°çµæœ")
    model2 = plot_simple_multiple_regression()
    
    print("\nğŸ‰ ãƒ‡ãƒ¢å®Œäº†!")
    print("\nğŸ“‹ é‡å›å¸°ã®ç‰¹å¾´:")
    print("- è¤‡æ•°ã®è¦å› ï¼ˆèª¬æ˜å¤‰æ•°ï¼‰ã‹ã‚‰äºˆæ¸¬")
    print("- ä¿‚æ•°ã®å¤§ãã•ã§å„è¦å› ã®å½±éŸ¿åº¦ãŒã‚ã‹ã‚‹")
    print("- RÂ²ã§äºˆæ¸¬ç²¾åº¦ã‚’è©•ä¾¡")
    print("- å®Ÿéš›ã®å·¥å ´ãƒ‡ãƒ¼ã‚¿ã«è¿‘ã„è¨­å®š")

if __name__ == "__main__":
    main() 