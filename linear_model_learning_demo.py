#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç·šå½¢ãƒ¢ãƒ‡ãƒ«å­¦ç¿’éç¨‹å¯è¦–åŒ–ãƒ‡ãƒ¢

ç·šå½¢å›å¸°ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆå‹¾é…é™ä¸‹æ³•ï¼‰ã‚’æ®µéšçš„ã«å¯è¦–åŒ–ã—ã¾ã™ã€‚
"""

import numpy as np
import matplotlib.pyplot as plt
import japanize_matplotlib
from matplotlib.animation import FuncAnimation
import pandas as pd
import warnings
import os
warnings.filterwarnings('ignore')

class LinearModelDemo:
    """ç·šå½¢ãƒ¢ãƒ‡ãƒ«å­¦ç¿’éç¨‹ã®ãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.history = []
        self.X = None
        self.y = None
        self.w = None
        self.b = None
        
    def generate_sample_data(self, n_samples=50, noise=0.5, seed=42):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        np.random.seed(seed)
        
        # çœŸã®é–¢ä¿‚: y = 2x + 1 + noise
        self.X = np.random.uniform(-2, 2, n_samples)
        self.y = 2 * self.X + 1 + np.random.normal(0, noise, n_samples)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–ï¼ˆé©å½“ãªå€¤ã‹ã‚‰é–‹å§‹ï¼‰
        self.w = np.random.uniform(-1, 1)  # é‡ã¿
        self.b = np.random.uniform(-1, 1)  # ãƒã‚¤ã‚¢ã‚¹
        
        print(f"çœŸã®é–¢ä¿‚: y = 2x + 1")
        print(f"åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: w={self.w:.3f}, b={self.b:.3f}")
        
    def predict(self, X, w=None, b=None):
        """äºˆæ¸¬"""
        if w is None:
            w = self.w
        if b is None:
            b = self.b
        return w * X + b
    
    def compute_loss(self, w=None, b=None):
        """å¹³å‡äºŒä¹—èª¤å·®ã‚’è¨ˆç®—"""
        if w is None:
            w = self.w
        if b is None:
            b = self.b
        
        y_pred = self.predict(self.X, w, b)
        loss = np.mean((self.y - y_pred) ** 2)
        return loss
    
    def compute_gradients(self):
        """å‹¾é…ã‚’è¨ˆç®—"""
        y_pred = self.predict(self.X)
        error = y_pred - self.y
        
        dw = np.mean(2 * error * self.X)  # âˆ‚L/âˆ‚w
        db = np.mean(2 * error)           # âˆ‚L/âˆ‚b
        
        return dw, db
    
    def train_step(self, learning_rate=0.01):
        """1ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’"""
        # å‹¾é…è¨ˆç®—
        dw, db = self.compute_gradients()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
        self.w -= learning_rate * dw
        self.b -= learning_rate * db
        
        # å±¥æ­´ä¿å­˜
        loss = self.compute_loss()
        self.history.append({
            'w': self.w,
            'b': self.b,
            'loss': loss,
            'dw': dw,
            'db': db
        })
        
        return loss
    
    def train(self, epochs=100, learning_rate=0.01):
        """å­¦ç¿’å®Ÿè¡Œ"""
        self.history = []
        
        for epoch in range(epochs):
            loss = self.train_step(learning_rate)
            
            if epoch % 20 == 0:
                print(f"Epoch {epoch:3d}: Loss={loss:.4f}, w={self.w:.3f}, b={self.b:.3f}")
        
        print(f"æœ€çµ‚: w={self.w:.3f}, b={self.b:.3f} (çœŸå€¤: w=2.0, b=1.0)")

def plot_learning_process_comprehensive():
    """åŒ…æ‹¬çš„ãªå­¦ç¿’éç¨‹ã®å¯è¦–åŒ–"""
    model = LinearModelDemo()
    model.generate_sample_data(n_samples=50, noise=0.3)
    
    # å­¦ç¿’å®Ÿè¡Œ
    model.train(epochs=100, learning_rate=0.1)
    
    # å¤§ããªå›³ã‚’ä½œæˆ
    fig = plt.figure(figsize=(20, 15))
    
    # 1. ãƒ‡ãƒ¼ã‚¿ã¨å­¦ç¿’çµæœ
    ax1 = plt.subplot(3, 3, 1)
    ax1.scatter(model.X, model.y, alpha=0.6, color='blue', label='å®Ÿãƒ‡ãƒ¼ã‚¿')
    
    # çœŸã®é–¢æ•°
    x_line = np.linspace(-2, 2, 100)
    y_true = 2 * x_line + 1
    ax1.plot(x_line, y_true, 'g--', linewidth=2, label='çœŸã®é–¢ä¿‚ (y=2x+1)')
    
    # å­¦ç¿’ã•ã‚ŒãŸé–¢æ•°
    y_learned = model.predict(x_line)
    ax1.plot(x_line, y_learned, 'r-', linewidth=2, label=f'å­¦ç¿’çµæœ (y={model.w:.2f}x+{model.b:.2f})')
    
    ax1.set_xlabel('x')
    ax1.set_ylabel('y')
    ax1.set_title('1. ãƒ‡ãƒ¼ã‚¿ã¨å­¦ç¿’çµæœ')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. æå¤±é–¢æ•°ã®å¤‰åŒ–
    ax2 = plt.subplot(3, 3, 2)
    losses = [h['loss'] for h in model.history]
    ax2.plot(losses, 'b-', linewidth=2)
    ax2.set_xlabel('ã‚¨ãƒãƒƒã‚¯')
    ax2.set_ylabel('æå¤± (MSE)')
    ax2.set_title('2. æå¤±é–¢æ•°ã®å¤‰åŒ–')
    ax2.grid(True, alpha=0.3)
    ax2.set_yscale('log')
    
    # 3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤‰åŒ–
    ax3 = plt.subplot(3, 3, 3)
    weights = [h['w'] for h in model.history]
    biases = [h['b'] for h in model.history]
    
    ax3.plot(weights, 'r-', linewidth=2, label='é‡ã¿ (w)')
    ax3.plot(biases, 'b-', linewidth=2, label='ãƒã‚¤ã‚¢ã‚¹ (b)')
    ax3.axhline(y=2, color='r', linestyle='--', alpha=0.7, label='çœŸã®é‡ã¿ (2.0)')
    ax3.axhline(y=1, color='b', linestyle='--', alpha=0.7, label='çœŸã®ãƒã‚¤ã‚¢ã‚¹ (1.0)')
    ax3.set_xlabel('ã‚¨ãƒãƒƒã‚¯')
    ax3.set_ylabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤')
    ax3.set_title('3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸ')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. å‹¾é…ã®å¤‰åŒ–
    ax4 = plt.subplot(3, 3, 4)
    dws = [h['dw'] for h in model.history]
    dbs = [h['db'] for h in model.history]
    
    ax4.plot(dws, 'r-', linewidth=2, label='âˆ‚L/âˆ‚w')
    ax4.plot(dbs, 'b-', linewidth=2, label='âˆ‚L/âˆ‚b')
    ax4.set_xlabel('ã‚¨ãƒãƒƒã‚¯')
    ax4.set_ylabel('å‹¾é…')
    ax4.set_title('4. å‹¾é…ã®å¤‰åŒ–')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. æå¤±é–¢æ•°ã®3Dè¡¨é¢
    ax5 = plt.subplot(3, 3, 5, projection='3d')
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã§ã®ã‚°ãƒªãƒƒãƒ‰
    w_range = np.linspace(-1, 4, 50)
    b_range = np.linspace(-2, 3, 50)
    W, B = np.meshgrid(w_range, b_range)
    
    # å„ç‚¹ã§ã®æå¤±ã‚’è¨ˆç®—
    Z = np.zeros_like(W)
    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
            Z[i, j] = model.compute_loss(W[i, j], B[i, j])
    
    ax5.plot_surface(W, B, Z, alpha=0.6, cmap='viridis')
    
    # å­¦ç¿’çµŒè·¯ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    w_path = [h['w'] for h in model.history]
    b_path = [h['b'] for h in model.history]
    loss_path = [h['loss'] for h in model.history]
    
    ax5.plot(w_path, b_path, loss_path, 'r-', linewidth=3, label='å­¦ç¿’çµŒè·¯')
    ax5.scatter([w_path[0]], [b_path[0]], [loss_path[0]], color='red', s=100, label='é–‹å§‹ç‚¹')
    ax5.scatter([w_path[-1]], [b_path[-1]], [loss_path[-1]], color='green', s=100, label='çµ‚äº†ç‚¹')
    
    ax5.set_xlabel('é‡ã¿ (w)')
    ax5.set_ylabel('ãƒã‚¤ã‚¢ã‚¹ (b)')
    ax5.set_zlabel('æå¤±')
    ax5.set_title('5. æå¤±é–¢æ•°ã®è¡¨é¢ã¨å­¦ç¿’çµŒè·¯')
    
    # 6. äºˆæ¸¬ã®æ”¹å–„éç¨‹
    ax6 = plt.subplot(3, 3, 6)
    ax6.scatter(model.X, model.y, alpha=0.6, color='blue', label='å®Ÿãƒ‡ãƒ¼ã‚¿')
    
    # å­¦ç¿’ã®å„æ®µéšã§ã®äºˆæ¸¬ç·šã‚’è¡¨ç¤º
    epochs_to_show = [0, 10, 30, 60, 99]
    colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']
    
    for i, epoch in enumerate(epochs_to_show):
        if epoch < len(model.history):
            w_epoch = model.history[epoch]['w']
            b_epoch = model.history[epoch]['b']
            y_pred_epoch = w_epoch * x_line + b_epoch
            ax6.plot(x_line, y_pred_epoch, color=colors[i], linewidth=2, 
                    alpha=0.7, label=f'ã‚¨ãƒãƒƒã‚¯ {epoch}')
    
    ax6.plot(x_line, y_true, 'k--', linewidth=2, label='çœŸã®é–¢ä¿‚')
    ax6.set_xlabel('x')
    ax6.set_ylabel('y')
    ax6.set_title('6. äºˆæ¸¬ã®æ”¹å–„éç¨‹')
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    
    # 7. å­¦ç¿’ç‡ã®å½±éŸ¿
    ax7 = plt.subplot(3, 3, 7)
    
    learning_rates = [0.001, 0.01, 0.1, 0.5]
    colors_lr = ['blue', 'green', 'orange', 'red']
    
    for lr, color in zip(learning_rates, colors_lr):
        model_lr = LinearModelDemo()
        model_lr.generate_sample_data(n_samples=50, noise=0.3, seed=42)
        model_lr.train(epochs=100, learning_rate=lr)
        
        losses_lr = [h['loss'] for h in model_lr.history]
        ax7.plot(losses_lr, color=color, linewidth=2, label=f'å­¦ç¿’ç‡ {lr}')
    
    ax7.set_xlabel('ã‚¨ãƒãƒƒã‚¯')
    ax7.set_ylabel('æå¤±')
    ax7.set_title('7. å­¦ç¿’ç‡ã®å½±éŸ¿')
    ax7.legend()
    ax7.grid(True, alpha=0.3)
    ax7.set_yscale('log')
    
    # 8. æ®‹å·®ã®åˆ†æ
    ax8 = plt.subplot(3, 3, 8)
    
    y_pred_final = model.predict(model.X)
    residuals = model.y - y_pred_final
    
    ax8.scatter(y_pred_final, residuals, alpha=0.6)
    ax8.axhline(y=0, color='red', linestyle='--')
    ax8.set_xlabel('äºˆæ¸¬å€¤')
    ax8.set_ylabel('æ®‹å·®')
    ax8.set_title('8. æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ')
    ax8.grid(True, alpha=0.3)
    
    # 9. å­¦ç¿’ã®æ•°å¼èª¬æ˜
    ax9 = plt.subplot(3, 3, 9)
    ax9.axis('off')
    
    formula_text = """
    ç·šå½¢å›å¸°ã®å­¦ç¿’éç¨‹
    
    1. ãƒ¢ãƒ‡ãƒ«: Å· = wx + b
    
    2. æå¤±é–¢æ•°: L = 1/n Î£(y - Å·)Â²
    
    3. å‹¾é…è¨ˆç®—:
       âˆ‚L/âˆ‚w = 2/n Î£(Å· - y)x
       âˆ‚L/âˆ‚b = 2/n Î£(Å· - y)
    
    4. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°:
       w â† w - Î±(âˆ‚L/âˆ‚w)
       b â† b - Î±(âˆ‚L/âˆ‚b)
    
    Î±: å­¦ç¿’ç‡
    """
    
    ax9.text(0.1, 0.9, formula_text, transform=ax9.transAxes, fontsize=12,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))
    
    plt.tight_layout()
    plt.show()
    
    return model

def plot_gradient_descent_animation():
    """å‹¾é…é™ä¸‹æ³•ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸ“¹ å‹¾é…é™ä¸‹æ³•ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
    
    model = LinearModelDemo()
    model.generate_sample_data(n_samples=30, noise=0.2)
    
    # å­¦ç¿’å®Ÿè¡Œ
    model.train(epochs=50, learning_rate=0.1)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    def animate(frame):
        ax1.clear()
        ax2.clear()
        
        if frame < len(model.history):
            # ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            current_w = model.history[frame]['w']
            current_b = model.history[frame]['b']
            current_loss = model.history[frame]['loss']
            
            # å·¦å´: ãƒ‡ãƒ¼ã‚¿ã¨ç¾åœ¨ã®äºˆæ¸¬ç·š
            ax1.scatter(model.X, model.y, alpha=0.6, color='blue', label='ãƒ‡ãƒ¼ã‚¿')
            x_line = np.linspace(-2, 2, 100)
            y_true = 2 * x_line + 1
            y_current = current_w * x_line + current_b
            
            ax1.plot(x_line, y_true, 'g--', linewidth=2, label='çœŸã®é–¢ä¿‚')
            ax1.plot(x_line, y_current, 'r-', linewidth=2, 
                    label=f'ç¾åœ¨: y={current_w:.2f}x+{current_b:.2f}')
            
            ax1.set_xlabel('x')
            ax1.set_ylabel('y')
            ax1.set_title(f'ã‚¨ãƒãƒƒã‚¯ {frame}: æå¤± = {current_loss:.4f}')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            ax1.set_xlim(-2, 2)
            ax1.set_ylim(-4, 6)
            
            # å³å´: æå¤±ã®å¤‰åŒ–
            losses = [h['loss'] for h in model.history[:frame+1]]
            ax2.plot(range(len(losses)), losses, 'b-', linewidth=2)
            ax2.scatter([frame], [current_loss], color='red', s=100, zorder=5)
            ax2.set_xlabel('ã‚¨ãƒãƒƒã‚¯')
            ax2.set_ylabel('æå¤±')
            ax2.set_title('æå¤±ã®å¤‰åŒ–')
            ax2.grid(True, alpha=0.3)
            ax2.set_xlim(0, len(model.history))
            ax2.set_ylim(0, max([h['loss'] for h in model.history]) * 1.1)
    
    # ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆï¼ˆè¡¨ç¤ºã®ã¿ï¼‰
    frames = min(50, len(model.history))
    for frame in range(0, frames, 5):  # 5ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«è¡¨ç¤º
        animate(frame)
        plt.pause(0.5)
    
    # æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ 
    animate(len(model.history) - 1)
    plt.show()

def plot_different_scenarios():
    """æ§˜ã€…ãªã‚·ãƒŠãƒªã‚ªã§ã®å­¦ç¿’"""
    print("ğŸ“Š æ§˜ã€…ãªã‚·ãƒŠãƒªã‚ªã§ã®å­¦ç¿’æ¯”è¼ƒ")
    
    scenarios = [
        {'noise': 0.1, 'title': 'ãƒã‚¤ã‚ºå°'},
        {'noise': 0.5, 'title': 'ãƒã‚¤ã‚ºä¸­'},
        {'noise': 1.0, 'title': 'ãƒã‚¤ã‚ºå¤§'},
    ]
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    for i, scenario in enumerate(scenarios):
        model = LinearModelDemo()
        model.generate_sample_data(n_samples=50, noise=scenario['noise'], seed=42)
        model.train(epochs=100, learning_rate=0.1)
        
        # ãƒ‡ãƒ¼ã‚¿ã¨çµæœ
        ax1 = axes[0, i]
        ax1.scatter(model.X, model.y, alpha=0.6, color='blue')
        x_line = np.linspace(-2, 2, 100)
        y_true = 2 * x_line + 1
        y_learned = model.predict(x_line)
        
        ax1.plot(x_line, y_true, 'g--', linewidth=2, label='çœŸã®é–¢ä¿‚')
        ax1.plot(x_line, y_learned, 'r-', linewidth=2, label='å­¦ç¿’çµæœ')
        ax1.set_title(f'{scenario["title"]}\næœ€çµ‚: w={model.w:.2f}, b={model.b:.2f}')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æå¤±ã®å¤‰åŒ–
        ax2 = axes[1, i]
        losses = [h['loss'] for h in model.history]
        ax2.plot(losses, 'b-', linewidth=2)
        ax2.set_xlabel('ã‚¨ãƒãƒƒã‚¯')
        ax2.set_ylabel('æå¤±')
        ax2.set_title(f'æå¤±å¤‰åŒ– (æœ€çµ‚: {losses[-1]:.4f})')
        ax2.grid(True, alpha=0.3)
        ax2.set_yscale('log')
    
    plt.tight_layout()
    plt.show()

def plot_simple_learning_result():
    """ãƒ‡ãƒ¼ã‚¿ã¨å­¦ç¿’çµæœã®ã‚·ãƒ³ãƒ—ãƒ«ãªè¡¨ç¤º"""
    model = LinearModelDemo()
    model.generate_sample_data(n_samples=50, noise=0.3)
    
    # å­¦ç¿’å®Ÿè¡Œ
    model.train(epochs=100, learning_rate=0.1)
    
    # imagesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs('images', exist_ok=True)
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªå›³ã‚’ä½œæˆ
    plt.figure(figsize=(10, 8))
    
    # ãƒ‡ãƒ¼ã‚¿ã¨å­¦ç¿’çµæœã®è¡¨ç¤º
    plt.scatter(model.X, model.y, alpha=0.7, color='blue', s=60, label='å®Ÿãƒ‡ãƒ¼ã‚¿', zorder=3)
    
    # å­¦ç¿’ã•ã‚ŒãŸé–¢æ•°ã®ã¿
    x_line = np.linspace(-2, 2, 100)
    y_learned = model.predict(x_line)
    plt.plot(x_line, y_learned, 'r-', linewidth=3, label='å­¦ç¿’çµæœ', zorder=2)
    
    plt.xlabel('x', fontsize=14)
    plt.ylabel('y', fontsize=14)
    plt.title('ã‚½ãƒ•ãƒˆã‚»ãƒ³ã‚µãƒ¼ã®å­¦ç¿’ã‚¤ãƒ¡ãƒ¼ã‚¸', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ç”»åƒã‚’ä¿å­˜
    filename = 'images/soft_sensor_learning.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“ ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
    
    plt.show()
    
    return model

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ“ ç·šå½¢ãƒ¢ãƒ‡ãƒ«å­¦ç¿’çµæœè¡¨ç¤º")
    print("=" * 40)
    
    model = plot_simple_learning_result()
    
    print("\nğŸ‰ å­¦ç¿’å®Œäº†!")
    print(f"ğŸ“Š çµæœ: w={model.w:.3f}, b={model.b:.3f}")
    print(f"ğŸ“‰ æœ€çµ‚æå¤±: {model.history[-1]['loss']:.4f}")

if __name__ == "__main__":
    main() 